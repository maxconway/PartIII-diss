% !TeX root = dissertation.Rnw   
\chapter{Methods}
\section{Data and Materials}
\subsection{Metabolic Models}
To conduct flux balance analysis, one needs a model of the metabolism of the organism in question. A number of exchange formats exist for these models and there are many more formats internal to tools, however after some investigation it was clear that the \em{de-facto} standard format is the use of tab-separated value tables, with the reactions specified in a space-separated format. 
\todo{describe column layout}
Unfortunately, the order and names of columns in these tables is not standardized, and the two Geobacter models that I used were supplied in an ad-hoc format, which was a problem given the inflexible import facilities of Matlab, and the fact that these tables could have cells with >250 characters. 
To fix this, I rewrote the import facilities in GDMO and GDLS in order to give them greater flexibility in the file formats that they could read.

The metabolic models used are listed in table~\ref{tab:models}. The columns used are as follows:
\begin{description}
\item[Species] The species which this metabolic model represents
\item[Shorthand] The shorthand text string used to represent this model in code, graphs and elsewhere
\item[Size] The number of reactions in the model
\item[Synthetic Objective] The material that we seek to overproduce.
\end{description}

\todo{needs citations}
\begin{table}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Species}         & \textbf{Shorthand}       & \textbf{Size} & \textbf{Synthetic Objective} \\ \hline
    Geobacter Sulfurreducens  & sulfurreducens  & 609  & \ce{FE2}            \\ \hline
    Geobacter Metallireducens & metallireducens & 788  & \ce{FE2}            \\ \hline
    Escherichia Coli          & iaf-1260        & 2383 & Acetate             \\ \hline
    Escherichia Coli          & iJO-1366        & 2583 & Acetate             \\ \hline
    \end{tabular}
    \label{tab:models}
\end{table}

\subsubsection{Sulfurreducens}
Described in 2009 in~\ref{Mahadevan2006}, this model helfully focusses on the \ce{FE3} reducing capabilities of Geobacter Sulfurreducens. 
Its primary energy source is chosen as Acetate, with traces of a number of other necessary compounds. 
\ce{FE2} is chosen as the target since due to the pilli that Geobacter possess, production of \ce{FE2}, can be considered to be biologically equivalent to electron transfer via an electrode\todo{check I've got this right way round}.
\ce{FE2} overproduction is chosen instead of \ce{FE3} overconsumption simply for uniformity---the two are equivalent, since Flux Balance Analysis rules out the organism having any net accumulation of Iron.

\subsubsection{Metallireducens}
This model~\ref{Sun2009} is based on a model of G. Sulfurreducens, but with the addition of a number of reactions that are not present in the base model. 

\subsubsection{iaf-1260}
iAF-1260~\ref{Feist2007} is a widely used reconstruction of the E. Coli metabolism.
Due to the extensive use of E. coli as a model organism, it is far better characterized than the Geobacteraceae, and hence 3-4 times as many reactions are known. As discussed in section~\ref{sex:complexity}, this results in much slower analysis of these networks.
\todo{couple E-coli and geobacter?}

\subsubsection{iJO-1366}
iJO1366~\ref{Orth2011} is newer and somewhat more comprehensive than iAF-1260.

\subsection{Optimization Libraries}
Flux Balance Analysis is a linear optimization problem, and can therefore be solved by the techniques of Mathematical Programming; in addtion, exhaustive enumeration of knockout strategies when coupled with Flux Balance Analysis is a Mixed Integer Programming problem. 
Various techniques exist to solve these problems, in particular the simplex algorithm for linear programming \todo{cite}. These techniques are extremely compute intensive, so simple Matlab libraries would be far too slow.
Instead, the common approach is to use Optimization Toolkits, which have a core of extremely high performance code in a low level language such as C\todo{cite}, with various wrappers to allow calls from more convenient languages like Matlab.

A number of these toolkits exist, with various papers discussing their relative merits \todo{cite}. The two used here are GNU Linear Programming Kit (GLPK) and Gurobi. 
\begin{description}
\item[GLPK] is free and open source and one of the most widely available toolkits, with interfaces to many languages on many platforms.
\item[Gurobi] is a commerical toolkit also with a good number of interfaces on all relevant platforms. The literature suggests that it is the fastest performing toolkit. \todo{cite}
\end{description}
The timing section~\ref{sec:complexity} is entirely based on GLPK. This is because initial experiments found it to perform faster for smaller problem sizes, despite the evidence that Gurobi is faster for large problems. Assuming that this is due to higher overheads in Gurobi, we can expect that measurments from small cases for GLPK will scale to large cases in a manner that is closer to theoretical predictions, allowing a more accurate model.

\subsection{Computing resources}

\subsection{Languages and programs}

\section{Overview of Genetic Design Techniques Studied}

\subsection{Genetic Design by Local Search (GDLS)}
%this all needs some nicer presentation
Genetic Design through Local Search~\cite{Lun2009}, or GDLS is an algorithm that is used to engineer organisms that produce particular metabolites. 
Given an FBA model with gene-protein reaction annotations, it produces a knockout strategy which will alter the metabolic network to overproduce the compound required.

GDLS is a local search algorithm, which means that it starts with a solution and recursively improves upon it. 
This is a sensible decision in metabolic engineering, due to the natural starting point of no knockouts. 
The detailed procedure is as follows:

\begin{algorithm}
\caption{Pseudocode of Genetic Design by Local Search}
\label{alg:GDLS}
\begin{algorithmic}
\State{\(X_{0} \gets \text{natural}\)}
\While{\(X_{i} \text{is better than} X_{i-1}\)}\State{
	\(X_{i+1} \gets \text{neighbourhood\_search} X_{i} \)
}
\EndWhile
\end{algorithmic}
\end{algorithm}
%need to expand this

This means that at each iteration, the algorithm starts with a candidate genome, or genomes. For each of these, it evaluates every genome that differs by at most neighbourhoodsize knockouts or knockins, and selects the best from these. The search then starts again from these best solutions, and this is repeated until no better solutions are found. 

\subsection{Genetic Design by Multi-objective Optimization(GDMO)} 
Genetic Design by Multi-objective Optimization \todo{who it's written by} is a genetic design algorithm based on the multi-objective evolutionary optimization algorithm NSGA-II.
Using a multi-objective optimization algorithm in this context has a number of advantages.
\begin{itemize}
\item A trade-off between different objectives does not need to be selected before use. This avoids the issue of having to guess the properties of a system that has not yet been designed.
\item The Pareto front can be used to visualize the space of possibilities. This can provide insight into the different structures created by different pathways, and help to design an organism that will not suffer from reduced production due to evolution once in use.
\item Crucially, this approach \todo{slower or faster?} does not have to sacrifice performance for these advantages, since maintaining a population of solutions that occupies a wide range of good phenotypes in fact helps the evolutionary process.
\end{itemize}

\section{Computational Complexity analysis}
\label{sex:complexity}
The core problem of this kind of genetic design (which does not use network structure information) is 0-1 integer linear programming~\cite{Karp1972}---indeed GDLS works by repeatedly solving small cases of this problem.
This means that this problem is NP-complete, and assuming \(\mathcal{P} \neq \mathcal{NP}\), these algorithms have running times that are not bounded by any polynomial.
This suggests extremely long running times for some combinations of input parameters.
This makes it important to be able to form an \emph{a priori} estimate of the running time, so that computation time is not wasted with infeasibly long runs.
These very long running times mean that forming a model purely by regression analysis over the whole parameter space would be impractical, and so a partly analytic approach must be taken, by examining the algorithm's procedures.

\subsection{Flux Balance Analysis}
Since FBA is at the core of the algorithms studied, it makes sense to consider this separately. 
Here, FBA is done via the Simplex linear programming algorithm, which has which takes \(O(n^3)\) steps for practical problems~\cite{Dantzig1963}. 
%A number of different solvers exist, with GLPK being used here, as the lowest common denominator.
%solvers vary in code speed by around 100-fold
In order to characterize the differences between running times for different models, FBA was performed 100 times with each or a number of models. The results, shown in figure~\ref{fig:FBAtimings}, show that times can vary by \todo{find out fold change} X-fold. This can be attributed to the variance in sizes of the different models used.

\begin{figure}
\label{fig:FBAtimings}
\centering
<<echo=FALSE>>=
FBAtimings <- read.csv("./experiments/FBAtimings.csv")
plot(time ~ strain, data=FBAtimings,
        range=0,
        xlab='Metabolic Model',
        ylab='time (s)'
        )
@
\caption{running times for FBA of metabolic models}
\end{figure}

\subsection{GDMO}
Since GDMO is an evolutionary algorithm, the number of objective function evaluations performed is proportional to the population size multiplied by the number of generations evaluated. 
Each objective evaluation requires that Flux Balance Analysis is conducted a constant number of times. 
This means that the total runtime can be modelled can be modelled via in R as the interaction of the three quantities of iterations, population and time to conduct one round of FBA.

%This is a time for a bit of sweave
%actually do stats
\begin{figure}
\centering
<<echo=FALSE>>=
data('GDMO')
GDMO = GDMO[,c('strain','cputime','generation','pop','id','FBAtime','walltime')]
GDMO = unique(GDMO)
GDMO$strain=as.factor(GDMO$strain)
GDMO = GDMO[sample(nrow(GDMO)),]
GDMO.train = head(GDMO,nrow(GDMO)*0.9)
GDMO.test = tail(GDMO,nrow(GDMO)*0.1)
GDMO.fit = lm(cputime~pop*generation*strain,GDMO.train)
GDMO.test$prediction=predict(GDMO.fit,newdata=GDMO.test)
plot(I(prediction/cputime)~strain,GDMO.test)
@
\end{figure}
\todo{label}
\todo{reference}

\subsubsection{Hypervolume computation}
Techniques to compare the output of multi-objective optimization algorithms is an area of active research. 
Ultimately, one cannot definitively say that pareto front A is better than pareto front B unless every point in B is dominated by a point in A. 
%equation
One relatively common metric for comparison of pareto fronts is the hypervolume enclosed by the front.
This enclosed hypervolume is smaller than the volume behind a hypersurface fitted to the front, which meant that the typical technique of fitting a curve to the pareto front and integrating this curve was inappropriate. Instead, I wrote a custom Monte Carlo integration function, which would work out the actual dominated area, while exploiting the performance advantages of Monte-Carlo techniques to allow for efficient use even on extremely large fronts. The following R code is used to calculated dominance relations.
\todo{might need listings package}

<<echo=TRUE>>=
dominates <- function(a,b){
  # does a dominate b?
  any(a>b)&!any(a<b)
}

dominators <- function(sol,frame){
  # how many rows in frame dominate sol?
  apply(frame,1,function(t){
    dominates(t,sol)
  })
}

nondominated <- function(x){
  # which rows of x are nondominated?
  # Returns:
  #  boolean vector
  apply(x,1,function(a){
    !any(dominators(a,x))
  })
}

fnondominated <- function(df){
  # which rows of x are nondominated?
  # Returns:
  #  data.frame
  df[nondominated(df),]
}
@
\todo{correct this}

\subsection{GDLS}
GDLS does not run to a hard limit, but instead stops when it finds the local optimum, so we cannot work out a useful upper bound on running time. 
A lower bound is, however, possible, and appears from exploratory experiments to be useful.
Four variables are used here to parametrize GDLS: 
\begin{enumerate}
\item The metabolic model used, let us call the number of potential knockouts in it \texttt{modelsize};
\item \texttt{nbhdsz}, number of knockouts distance from a solution that is exhaustively searched;
\item \texttt{M}, the number of search paths to maintain at each stage; and
\item \texttt{knockouts}, the maximum total number of knockouts (fixed at 40 for fair comparison with GDMO).
\end{enumerate}
Here we can see that the metabolic model used will have a very large effect, since not only does this effect the time to complete one FBA evaluation, but the space that much be searched around a given solution is \(\texttt{modelsize}^\texttt{neighbourhoodsize}\). This means that, total search time for one solution is roughly \(\texttt{modelsize}^(3+\texttt{neighbourhoodsize}\), once the time to conduct FBA is included. Overall, this gives a lower bound on running time of \(\texttt{M} * \texttt{modelsize} ^ ( 3 + \texttt{neighbourhoodsize}\).
\todo{is branching factor number of search paths, or is it at each stage?}

To find the relative effects of these parameters on execution time, the trials listed in table~\ref{tab:GDLS} were performed.
<<GDLSevals,results='asis'>>=
data(GDLS)
require(xtable,quietly=TRUE)
GDLS.runs=aggregate(GDLS[,c('strain','cputime','walltime','iterations','id','nbhdsz','M','FBAtime')],by=list(GDLS$id),max)
print(xtable(GDLS.runs[,c('strain','cputime','nbhdsz','M')],caption='GDLS evaluations completed',label='tab:GDLS'))
@

\section{Instrumentation}
To facilitate comparison, the genetic design strategies were instrumented in order to record every candidate solution that they produced. This allows the improvement of the candidates over time to be seen. 
Tests were conducted to ensure that this instrumentation code did not influence the results by its own execution time; it was found to run fast enough that the slowdown was undetectable.
