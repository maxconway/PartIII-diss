% !TeX root = dissertation.Rnw   
\chapter{Methods}
The Methods chapter describes the Biological models used, some of the more technical decisions made, and presents the analysis performed in order to predict the impact of parameter selections on execution time.

\section{Metabolic Models}
To conduct flux balance analysis, one needs a model of the metabolism of the organism in question. A number of exchange formats exist for these models and there are many more formats internal to tools. However, after some investigation it was clear that the de facto standard format is the use of tab-separated value tables, with the reactions specified in a space-separated format. 

Unfortunately, the order and names of columns in these tables is not standardized, and the two \textit{Geobacter} models used were supplied in an ad-hoc format, which was a problem given the inflexible import facilities of Matlab, and the fact that these tables could have cells with \(>250\) characters. To fix this, the import facilities were rewritten in GDMO and GDLS in order to give them greater flexibility in the file formats that they could read.

The metabolic models used are listed in table~\ref{tab:models}. The columns used are as follows:
\begin{description}
\item[Species] The species which this metabolic model represents.
\item[Shorthand] The shorthand text string used to represent this model in code, graphs and elsewhere.
\item[Size] The number of reactions in the model.
\item[Synthetic Objective] The material that we seek to overproduce.
\end{description}

\begin{table}
    \begin{tabular}{ l l l l l}
    \hline
    Species         & Shorthand       & Size & Synthetic Objective \\ \hline
    \textit{Geobacter sulfurreducens}  & sulfurreducens  & 609  & \ce{FE2}            \\ 
    \textit{Geobacter metallireducens} & metallireducens & 788  & \ce{FE2}            \\ 
    \textit{Escherichia coli}          & iaf1260        & 2383 & Acetate             \\ 
    \textit{Escherichia coli}         & iJO1366-aerobic        & 2583 & Acetate             \\
    \textit{Escherichia coli}          & iJO1366-anaerobic        & 2583 & Acetate             \\ \hline
    \end{tabular}
    \caption{Metabolic Models}
    \label{tab:models}
\end{table}

\subsection{Sulfurreducens}
Described in 2009 in~\cite{Mahadevan2006}, this model focuses on the \ce{FE3} reducing capabilities of \textit{Geobacter sulfurreducens}. 
Its primary energy source is chosen as acetate, with traces of a number of other necessary compounds. 
\ce{FE2} is chosen as the target since due to the pilli that \textit{Geobacter} possess, production of \ce{FE2}, can be considered to be biologically equivalent to electron transfer via an electrode.
\ce{FE2} overproduction is chosen instead of \ce{FE3} overconsumption simply for uniformity---the two are equivalent, since Flux Balance Analysis rules out the organism having any net accumulation of Iron.

\subsection{Metallireducens}
This model~\cite{Sun2009} is based on a model of \textit{G. sulfurreducens}, but with the addition of a number of reactions that are present in G. Metallireducens but not \textit{G. sulfurreducens}. 

\subsection{iaf1260}
iAF1260~\cite{Feist2007} is a widely used reconstruction of the \textit{Escherichia coli} metabolism.
Due to the extensive use of \textit{E. coli} as a model organism, it is far better characterized than \textit{Geobacter}, and hence 3-4 times as many reactions are known. 
As discussed in section~\ref{sec:complexity}, this results in much slower analysis of these networks.
This reconstruction of \textit{E. coli} has been used extensively\footnote{218 citations for iAF1260 are listed by Google Scholar, versus 28 for iJO1366}, and so it was used in this project for testing and validation; specifically it allowed problems to be narrowed down to being either with the optimization algorithm, or the model being optimized.

\subsection{iJO1366}
iJO1366~\cite{Orth2011} is newer and somewhat more comprehensive than iAF1260.
Since iJO1366 contains more reactions and more comprehensive gene-protein-reaction annotations, sub-systems from this were used for knock-in investigations.

\subsection{Model Reduction}
In order to increase execution speed and clarify the results, the model reduction technique designed for use with the GDLS~\cite{Lun2009} procedure was used. This iteratively removed dead end reactions and merged reactions that uniquely shared a metabolite until no more simplifications were possible, resulting in final models with sizes \SIrange{44}{80}{\percent} of the original, without loss of descriptive power.

\section{Computational Decisions}
\subsection{Optimization Libraries}
Flux Balance Analysis is a linear optimization problem, and can therefore be solved by the techniques of Mathematical Programming; in addition, exhaustive enumeration of knockout strategies when coupled with Flux Balance Analysis is a Mixed Integer Programming problem. 
Various techniques exist to solve these problems, in particular the Simplex algorithm for linear programming mentioned in section~\ref{sec:FBA}. These techniques are extremely compute intensive, so simple Matlab libraries would be far too slow.
Instead, the common approach is to use Optimization Toolkits, which have a core of extremely high performance code in a low level language such as C, with various wrappers to allow calls from more convenient languages like Matlab.

A number of these toolkits exist, with various papers discussing their relative merits~\cite{Meindl2012a}. The two used here are GNU Linear Programming Kit (GLPK) and Gurobi. 
\begin{description}
\item[GLPK] is free and open source and one of the most widely available toolkits, with interfaces to many languages on many platforms.
\item[Gurobi] is a commercial toolkit also with a good number of interfaces on all relevant platforms. The literature suggests that it is the fastest performing toolkit~\cite{Meindl2012}.
\end{description}
The timing section~\ref{sec:complexity} is entirely based on GLPK. This is because initial experiments found it to perform faster for smaller problem sizes, despite the evidence that Gurobi is faster for large problems. Assuming that this is due to higher overheads in Gurobi, we can expect that measurements from small cases for GLPK will scale to large cases in a manner that is closer to theoretical predictions, allowing a more accurate model.

\subsection{Languages and Programs}
The genetic design algorithms used were written in Matlab, while analysis was conducted in R to take advantage of the safety and reproducibility provided by its stricter type system.

\subsection{Pipeline}
The Make build system was used to automate a pipeline that went from the human readable metabolic model description files right through to the final analysis.
This pipeline is shown in figure~\ref{fig:pipeline}.

\begin{figure}[h]
\includegraphics[width=\linewidth]{pipeline.pdf}
\caption{The pipeline used in this project, with the associated file suffixes}
\label{fig:pipeline}
\end{figure}

\subsection{Hypervolume Computation}
\label{hypervolume}
Techniques to compare the output of multi-objective optimization algorithms are an area of active research~\cite{Okabe2003}. 
Ultimately, one cannot definitively say that Pareto front A is better than Pareto front B unless every point in B is dominated by a point in A. 
One relatively common metric for comparison of Pareto fronts is the hypervolume enclosed by the front~\cite{Zitzler1999,Zitzler}. 
This can be defined as the volume of the space for which points \(p\) obey \(\exists s . (s \in \mathrm{Front} \land s \succ p)\).
This has the properties of providing some measure of both spread and optimality of solutions, and reducing to the product of the objectives with only one solution. 

Since hypervolume increases with density of solutions, and with their magnitude, it only makes sense to use hypervolume when we aim to maximize all objectives; this is already the case with this research, but nevertheless all objectives will still be normalized against the wild-type phenotype, to avoid undue influence from their relative absolute magnitudes.

While libraries exist that can find the volume of a point cloud efficiently, the volume that they find is generally not the dominated volume that we desire, but instead the volume of a mesh, or of the convex hull. 
While the difference between the volume of a mesh fitted to the points and the dominated volume may potentially be small, it is this small difference that provides a measurement of solution density, and so these approximations cannot be tolerated.

For this reason, I wrote a custom Monte Carlo integration function, which could calculate the hypervolume to any desired degree of accuracy. This was further optimized by a deterministic bounding box---this quickly partitions the objective space into sections:
\begin{enumerate}
\item the volume dominated by all solutions\label{enum:alldom},
\item the volume dominated by some of the solutions, and\label{enum:somedom}
\item the volume that dominates all the solutions.\label{enum:nodom}
\end{enumerate}
We know that volume~\ref{enum:alldom} should be included in the hypervolume and that volume~\ref{enum:nodom} should not, so the search space can be narrowed down to just volume~\ref{enum:somedom}.
The domination calculation itself was optimized by a short circuit evaluation: the majority of samples taken in the Monte Carlo~\cite{Metropolis1949} simulation either dominate many solutions or very few, so many can be found to be inside the dominated volume by comparison to only a small number of Pareto-optimal points.

The following code chunks show how this algorithm works (see Section~\ref{sec:embeddedcode}).

<<dominated,cache=TRUE>>=
dominates <- function(a,b){
  # does a dominate b?
  any(a>b)&!any(a<b)
}

#helper function
singledom <- function(p,front){
  stopifnot(is.data.frame(p) 
            & nrow(p)==1
            )  # this also covers NULL
  for(i in 1:nrow(front)){
    a=front[i,]
    if(dominates(a,p)){
      return(TRUE)
    }
  } 
  return(FALSE)
}

dominated <- function(p,front){
  # if on arg, return self dominance
  if(nargs()==1){
    return(dominated(p,p))
  }
  
  # if p is a point, points in front that dominate p
  if(!nrow(p)>1){
    return(apply(front,1,function(f){
      dominates(f,p)
    }))
  }
  
  # if p is a set of points, is each point dominated?
  if(is.data.frame(p) & nrow(p)>1){
    return(apply(p,1,function(a){
      singledom(a,front)
    }))
  }
}
@

<<hypervolumeMonteCarlo,echo=T,dependson=c('dominated'),cache=TRUE>>=
hypervolumeMonteCarlo <- function(x,num){
  range<-function(x){
    return(max(x)-min(x))
  }
  # deterministic portion
  excluded=range(x$nbiomass)*range(x$nmaxsyn)*range(x$nminsyn)
  included=max(x$nbiomass)*max(x$nmaxsyn)*max(x$nminsyn) - excluded
  
  #stochastic portion
  points=data.frame(nbiomass=runif(num,
                                   min(x$nbiomass),
                                   max(x$nbiomass)),
                    nmaxsyn =runif(num,
                                   min(x$nmaxsyn),
                                   max(x$nmaxsyn)),
                    nminsyn =runif(num,
                                   min(x$nminsyn),
                                   max(x$nminsyn)))
  a=dominated(points,x)
  
  return(excluded*sum(a)/num + included)
}
@

\section{Computational Complexity Analysis}
\label{sec:complexity}
The core problem of this kind of genetic design (which does not use network structure information) is 0-1 integer linear programming~\cite{Karp1972}---indeed GDLS works by repeatedly solving small cases of this problem.
This means that this problem is NP-complete, and assuming \( \P \neq \NP \), these algorithms have running times that are not bounded by any polynomial.
This suggests extremely long running times for some combinations of input parameters.
This makes it important to be able to form an a priori estimate of the running time, so that computation time is not wasted with infeasibly long runs.
These very long running times mean that forming a model purely by regression analysis over the whole parameter space would be impractical, and so a partly analytic approach must be taken, by examining the algorithm's procedures.

\subsection{Flux Balance Analysis}
Since FBA is at the core of the algorithms studied, it makes sense to consider this separately. 
Here, FBA is done via the Simplex linear programming algorithm, which takes \(O(n^3)\) steps for practical problems~\cite{Dantzig1963}. 
%A number of different solvers exist, with GLPK being used here, as the lowest common denominator.
%solvers vary in code speed by around 100-fold
In order to characterize the differences between running times for different models, FBA was performed 100 times with each or a number of models. The results, shown in figure~\ref{fig:FBAtimings}, show that times can vary by 35-fold. This can be attributed to the variance in sizes of the different models used.

<<FBAtimings,fig.height=10,fig.cap='Running times for FBA of metabolic models',echo=FALSE>>=
data('FBAtimings')
default=par(mar=c(10,6,6,2)+0.1)
plot(time ~ strain, data=FBAtimings,
     range=0,
     xlab='',
     ylab='time per round of FBA (s)',
     las=2
)
par(default)
@

\subsection{GDMO}
Since GDMO is an evolutionary algorithm, the number of objective function evaluations performed is proportional to the population size multiplied by the number of generations evaluated. 
Each objective evaluation requires that Flux Balance Analysis is conducted a constant number of times. 
This means that the total runtime can be modelled  as the interaction of the three quantities of iterations, population and time to conduct one round of FBA.

<<methods_GDMO_timings1,cache=TRUE>>=
data('GDMO') 
GDMO.timing = GDMO[,c('strain','generation','pop','id',
                      'FBAtime','walltime','cputime'
                      )]
GDMO.timing = unique(GDMO.timing)
GDMO.timing = GDMO.timing[sample(nrow(GDMO.timing)),]
GDMO.train = head(GDMO.timing,nrow(GDMO.timing)*0.9)
GDMO.test = tail(GDMO.timing,nrow(GDMO.timing)*0.1)
colourize=function(v){
  as.character(
  sapply(v,function(x){
    if(x=='sulfurreducens'){return('red')}
    if(x=='metallireducens'){return('blue')}
    if(x=='iaf1260-ac'){return('green')}
  })
  )
}
@

<<methods_GDMO_timings2,fig.cap='Predicted runtime, using time for 1 FBA round',dependson='methods_GDMO_timings1',out.height='0.4\\textheight',fig.pos='h',dev='png'>>=
GDMO.fit = lm(cputime~pop:generation:FBAtime,GDMO.train)
GDMO.test$prediction=predict(GDMO.fit,newdata=GDMO.test)
plot(prediction~cputime,
     GDMO.test, 
     col=colourize(GDMO.test$strain),
     xlab='actual cputime (s)',
     ylab='predicted cputime (s)')
legend('topright',
       legend=unique(GDMO.test$strain),
       col=colourize(unique(GDMO.test$strain)),
       pch='O'
)
@
Figure~\ref{fig:methods_GDMO_timings2} shows the predictions produced by this linear model, which uses the time taken to conduct one round of FBA.
Clearly, the prediction holds well within each strain, but there are big differences between strains.
This indicates that time to conduct Flux Balance Analysis is not, in fact, a good predictor between strains, despite its consistency within strains. Luckily, this in-silico data is available in abundance, so it was possible to get round this problem by simply fitting four separate models, as shown in figure~\ref{fig:methods_GDMO_timings3}.

<<methods_GDMO_timings3,fig.cap='Predicted runtime, conditional on strain',dependson='methods_GDMO_timings1',out.height='0.4\\textheight',fig.pos='h',dev='png'>>=
GDMO.fit<-lm(cputime~pop:generation:strain,GDMO.train) 
GDMO.test$prediction<-predict(GDMO.fit,newdata=GDMO.test)
plot(prediction~cputime,
     GDMO.test,
     col=colourize(GDMO.test$strain),
     xlab='actual cputime (s)',
     ylab='predicted cputime (s)')
legend('topright',
       legend=unique(GDMO.test$strain),
       col=colourize(unique(GDMO.test$strain)),
       pch='O'
)
@

\subsection{GDLS}
GDLS does not run to a hard limit, but instead stops when it finds the local optimum, so we cannot work out a useful upper bound on running time. 
A lower bound is, however, possible, and appears from exploratory experiments to be useful.
Four variables are used here to parametrize GDLS: 
\begin{enumerate}
\item The metabolic model used, let us call the number of potential knockouts in it \texttt{modelsize};
\item \texttt{nbhdsz}, number of knockouts distance from a solution that is exhaustively searched;
\item \texttt{M}, the number of search paths to maintain at each stage; and
\item \texttt{knockouts}, the maximum total number of knockouts (fixed at 40 for fair comparison with GDMO).
\end{enumerate}
Here we can see that the metabolic model used will have a very large effect, since not only does this effect the time to complete one FBA evaluation, but the space that much be searched around a given solution is \(\texttt{modelsize} ^ \texttt{nbhdsz}\). This means that, total search time for one solution is roughly \(\texttt{modelsize}^{(3+\texttt{nbhdsz})}\), once the time to conduct FBA is included. Overall, this gives a lower bound on running time of \(\texttt{M} * \texttt{modelsize} ^{ ( 3 + \texttt{nbhdsz})}\).

Although G. Sulfurreducens was used for performing the comparison of algorithms, other testing showed that model size did indeed have by far the largest effect on the execution time of GDLS.

\section{Instrumentation}
To facilitate comparison, the genetic design strategies were instrumented in order to record every candidate solution that they produced. This allows the improvement of the candidates over time to be seen. 
Tests were conducted to ensure that this instrumentation code did not influence the results by its own execution time; it was found to run fast enough that the slowdown was not detectable above noise.
