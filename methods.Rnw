% !TeX root = dissertation.Rnw   
\chapter{Methods}
\section{Data and Materials}
\subsection{Metabolic Models}
To conduct flux balance analysis, one needs a model of the metabolism of the organism in question. A number of exchange formats exist for these models and there are many more formats internal to tools, however after some investigation it was clear that the de-facto standard format is the use of tab-separated value tables, with the reactions specified in a space-separated format. 
\todo{describe column layout}
Unfortunately, the order and names of columns in these tables is not standardized, and the two Geobacter models that I used were supplied in an ad-hoc format, which was a problem given the inflexible import facilities of Matlab, and the fact that these tables could have cells with >250 characters. 
To fix this, I rewrote the import facilities in GDMO and GDLS in order to give them greater flexibility in the file formats that they could read.

The metabolic models used are listed in table~\ref{tab:models}. The columns used are as follows:
\begin{description}
\item[Species] The species which this metabolic model represents
\item[Shorthand] The shorthand text string used to represent this model in code, graphs and elsewhere
\item[Size] The number of reactions in the model
\item[Synthetic Objective] The material that we seek to overproduce.
\end{description}

\todo{discuss model reduction}

\todo{table needs citations}
\begin{table}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Species}         & \textbf{Shorthand}       & \textbf{Size} & \textbf{Synthetic Objective} \\ \hline
    Geobacter Sulfurreducens  & sulfurreducens  & 609  & \ce{FE2}            \\ \hline
    Geobacter Metallireducens & metallireducens & 788  & \ce{FE2}            \\ \hline
    Escherichia Coli          & iaf-1260        & 2383 & Acetate             \\ \hline
    Escherichia Coli          & iJO-1366        & 2583 & Acetate             \\ \hline
    \end{tabular}
    \label{tab:models}
\end{table}

\subsubsection{Sulfurreducens}
Described in 2009 in~\ref{Mahadevan2006}, this model helfully focusses on the \ce{FE3} reducing capabilities of Geobacter Sulfurreducens. 
Its primary energy source is chosen as Acetate, with traces of a number of other necessary compounds. 
\ce{FE2} is chosen as the target since due to the pilli that Geobacter possess, production of \ce{FE2}, can be considered to be biologically equivalent to electron transfer via an electrode\todo{check I've got this right way round}.
\ce{FE2} overproduction is chosen instead of \ce{FE3} overconsumption simply for uniformity---the two are equivalent, since Flux Balance Analysis rules out the organism having any net accumulation of Iron.

\subsubsection{Metallireducens}
This model~\ref{Sun2009} is based on a model of G. Sulfurreducens, but with the addition of a number of reactions that are not present in the base model. 

\subsubsection{iaf-1260}
iAF1260~\ref{Feist2007} is a widely used reconstruction of the E. Coli metabolism.
Due to the extensive use of E. coli as a model organism, it is far better characterized than the Geobacteraceae, and hence 3-4 times as many reactions are known. As discussed in section~\ref{sex:complexity}, this results in much slower analysis of these networks.
\todo{couple E-coli and geobacter?}

\subsubsection{iJO-1366}
iJO1366~\ref{Orth2011} is newer and somewhat more comprehensive than iAF1260.

\subsection{Optimization Libraries}
Flux Balance Analysis is a linear optimization problem, and can therefore be solved by the techniques of Mathematical Programming; in addtion, exhaustive enumeration of knockout strategies when coupled with Flux Balance Analysis is a Mixed Integer Programming problem. 
Various techniques exist to solve these problems, in particular the simplex algorithm for linear programming \todo{cite}. These techniques are extremely compute intensive, so simple Matlab libraries would be far too slow.
Instead, the common approach is to use Optimization Toolkits, which have a core of extremely high performance code in a low level language such as C\todo{cite}, with various wrappers to allow calls from more convenient languages like Matlab.

A number of these toolkits exist, with various papers discussing their relative merits \todo{cite}. The two used here are GNU Linear Programming Kit (GLPK) and Gurobi. 
\begin{description}
\item[GLPK] is free and open source and one of the most widely available toolkits, with interfaces to many languages on many platforms.
\item[Gurobi] is a commerical toolkit also with a good number of interfaces on all relevant platforms. The literature suggests that it is the fastest performing toolkit. \todo{cite}
\end{description}
The timing section~\ref{sec:complexity} is entirely based on GLPK. This is because initial experiments found it to perform faster for smaller problem sizes, despite the evidence that Gurobi is faster for large problems. Assuming that this is due to higher overheads in Gurobi, we can expect that measurments from small cases for GLPK will scale to large cases in a manner that is closer to theoretical predictions, allowing a more accurate model.

\subsection{Computing resources}

\subsection{Languages and programs}
The genetic design algorithms used were written in Matlab, while analysis was conducted in R due to its superior text handling abilities.
Git was used for version control, while the parsing and reduction of metabolic models was automated using the Make build system.

\section{Overview of Genetic Design Techniques Studied}

\subsection{Genetic Design by Local Search (GDLS)}
%this all needs some nicer presentation
Genetic Design through Local Search~\cite{Lun2009}, or GDLS is an algorithm that is used to engineer organisms that produce particular metabolites. 
Given an FBA model with gene-protein reaction annotations, it produces a knockout strategy which will alter the metabolic network to overproduce the compound required.

GDLS is a local search algorithm, which means that it starts with a solution and recursively improves upon it. 
This is a sensible decision in metabolic engineering, due to the natural starting point of no knockouts. 
The detailed procedure is as follows:

\begin{algorithm}
\caption{Pseudocode of Genetic Design by Local Search}
\label{alg:GDLS}
\begin{algorithmic}
\State{\(X_{0} \gets \text{natural}\)}
\While{\(X_{i} \text{is better than} X_{i-1}\)}\State{
	\(X_{i+1} \gets \text{neighbourhood\_search} X_{i} \)
}
\EndWhile
\end{algorithmic}
\end{algorithm}
%need to expand this

This means that at each iteration, the algorithm starts with a candidate genome, or genomes. For each of these, it evaluates every genome that differs by at most neighbourhoodsize knockouts or knockins, and selects the best from these. The search then starts again from these best solutions, and this is repeated until no better solutions are found. 

\subsection{Genetic Design by Multi-objective Optimization(GDMO)} 
Genetic Design by Multi-objective Optimization \todo{who it's written by} is a genetic design algorithm based on the multi-objective evolutionary optimization algorithm NSGA-II.
Using a multi-objective optimization algorithm in this context has a number of advantages.
\begin{itemize}
\item A trade-off between different objectives does not need to be selected before use. This avoids the issue of having to guess the properties of a system that has not yet been designed.
\item The Pareto front can be used to visualize the space of possibilities. This can provide insight into the different structures created by different pathways, and help to design an organism that will not suffer from reduced production due to evolution once in use.
\item Crucially, this approach \todo{slower or faster?} does not have to sacrifice performance for these advantages, since maintaining a population of solutions that occupies a wide range of good phenotypes in fact helps the evolutionary process.
\end{itemize}

\section{Computational Complexity analysis}
\label{sex:complexity}
The core problem of this kind of genetic design (which does not use network structure information) is 0-1 integer linear programming~\cite{Karp1972}---indeed GDLS works by repeatedly solving small cases of this problem.
This means that this problem is NP-complete, and assuming \(\mathcal{P} \neq \mathcal{NP}\), these algorithms have running times that are not bounded by any polynomial.
This suggests extremely long running times for some combinations of input parameters.
This makes it important to be able to form an a priori estimate of the running time, so that computation time is not wasted with infeasibly long runs.
These very long running times mean that forming a model purely by regression analysis over the whole parameter space would be impractical, and so a partly analytic approach must be taken, by examining the algorithm's procedures.

\subsection{Flux Balance Analysis}
Since FBA is at the core of the algorithms studied, it makes sense to consider this separately. 
Here, FBA is done via the Simplex linear programming algorithm, which has which takes \(O(n^3)\) steps for practical problems~\cite{Dantzig1963}. 
%A number of different solvers exist, with GLPK being used here, as the lowest common denominator.
%solvers vary in code speed by around 100-fold
In order to characterize the differences between running times for different models, FBA was performed 100 times with each or a number of models. The results, shown in figure~\ref{fig:FBAtimings}, show that times can vary by \todo{find out fold change} X-fold. This can be attributed to the variance in sizes of the different models used.

\begin{figure}
\label{fig:FBAtimings}
\centering
<<echo=FALSE>>=
data('FBAtimings')
plot(time ~ strain, data=FBAtimings,
        range=0,
        xlab='Metabolic Model',
        ylab='time (s)'
        )
@
\caption{running times for FBA of metabolic models}
\end{figure}

\subsection{GDMO}
Since GDMO is an evolutionary algorithm, the number of objective function evaluations performed is proportional to the population size multiplied by the number of generations evaluated. 
Each objective evaluation requires that Flux Balance Analysis is conducted a constant number of times. 
This means that the total runtime can be modelled  as the interaction of the three quantities of iterations, population and time to conduct one round of FBA.

<<methods_GDMO_timings1,cache=TRUE>>=
data('GDMO')
GDMO.timing = GDMO[,c('strain','generation','pop','id','FBAtime','walltime','cputime')]
GDMO.timing = unique(GDMO.timing)
GDMO.timing = GDMO.timing[sample(nrow(GDMO.timing)),]
GDMO.train = head(GDMO.timing,nrow(GDMO.timing)*0.9)
GDMO.test = tail(GDMO.timing,nrow(GDMO.timing)*0.1)
@

<<methods_GDMO_timings2,fig.cap='predicted time taken using time for 1 FBA round',dependson='methods_GDMO_timings1'>>=
GDMO.fit = lm(cputime~pop:generation:FBAtime,GDMO.train)
GDMO.test$prediction=predict(GDMO.fit,newdata=GDMO.test)
plot(prediction~cputime,
     GDMO.test,
     col=GDMO.test$strain,
     xlab='actual cputime (s)',
     ylab='predicted cputime (s)')
legend('topright',
       legend=unique(GDMO.test$strain),
       col=unique(GDMO.test$strain),pch='O')
@
Figure~\ref{fig:methods_GDMO_timings2} shows the predictions produced by this linear model, which uses the time taken to conduct one round of FBA.
Clearly, the prediction holds well within each strain, but there are big differences between strains.
This indicates that time to conduct Flux Balance Analysis is not, in fact, a good predictor between strains, despite its consistency wihtin strains. Luckily, this in-silico data is available in abundance, so it was possible to get round this problem by simply fitting four separate models, as shown in figure~\ref{fig:methods_GDMO_timings3}.

<<methods_GDMO_timings3,fig.cap='predicted time taken conditional on strain',dependson='methods_GDMO_timings1'>>=
GDMO.fit = lm(cputime~pop:generation:strain,GDMO.train)
GDMO.test$prediction=predict(GDMO.fit,newdata=GDMO.test)
plot(prediction~cputime,
     GDMO.test,
     col=GDMO.test$strain,
     xlab='actual cputime (s)',
     ylab='predicted cputime (s)')
legend('topleft',
       legend=unique(GDMO.test$strain),
       col=unique(GDMO.test$strain),pch='O')
@

\subsubsection{Hypervolume computation}
Techniques to compare the output of multi-objective optimization algorithms is an area of active research. 
Ultimately, one cannot definitively say that pareto front A is better than pareto front B unless every point in B is dominated by a point in A. 
%equation
One relatively common metric for comparison of pareto fronts is the hypervolume enclosed by the front. 
This provides some measure of both spread and optimality of solutions, and reduces to the product of the objectives with only one solution. 
Since hypervolume increases with spread of solutions, it only makes sense to use when we aim to maximize all objectives; this is already the case with this research, but nevertheless all objectives will still be normalized against the wild-type phenotype, to avoid undue influence from their relative absolute magnitudes.

Off the shelf techniques for finding volumes of point clouds tend to find the volume of the convex hull, which is larger than the dominated volume, and in general the efficient calculation of volumes of point clouds is an area of active research in Computer Graphics.\todo{cite}

To avoid some of these problems, I wrote a custom Monte Carlo integration function, which could calculate the hypervolume to any desired degree of accuracy. This was further optimized by a deterministic bounding box---this quickly partitions the objective space into sections:
\begin{enumerate}
\item the volume dominated by all solutions\label{enum:alldom},
\item the volume dominated by some of the solutions and\label{enum:somedom}
\item the volume that dominates all the solutions.\label{enum:nodom}
\end{enumerate}
We know that volume~\ref{enum:alldom} should be included in the hypervolume and that volume~\ref{enum:nodom} should not, so the search space can be narrowed down to just volume~\ref{enum:somedom}.
The domination calculation itself was optimized by a short circuit evaluation: the majority of samples taken in the Monte Carlo simulation either dominate many solutions or very few, so many can be found to be inside the dominated volume by comparison to only a small number of pareto-optimal points.

<<dominated,echo=F,cache=TRUE>>=
dominates <- function(a,b){
  # does a dominate b?
  any(a>b)&!any(a<b)
}

#helper function
singledom <- function(p,front){
  stopifnot(is.data.frame(p) 
            & nrow(p)==1
            )  # this also covers NULL
  for(i in 1:nrow(front)){
    a=front[i,]
    if(dominates(a,p)){
      return(TRUE)
    }
  } 
  return(FALSE)
}

dominated <- function(p,front){
  # if on arg, return self dominance
  if(nargs()==1){
    return(dominated(p,p))
  }
  
  # if p is a point, points in front that dominate p
  if(!nrow(p)>1){
    return(apply(front,1,function(f){
      dominates(f,p)
    }))
  }
  
  # if p is a set of points, is each point dominated?
  if(is.data.frame(p) & nrow(p)>1){
    return(apply(p,1,function(a){
      singledom(a,front)
    }))
  }
}
@

<<hypervolumeMonteCarlo,echo=T,dependson=c('dominated'),cache=TRUE>>=
hypervolumeMonteCarlo <- function(x,num){
  range<-function(x){
    return(max(x)-min(x))
  }
  # deterministic portion
  excluded=range(x$nbiomass)*range(x$nmaxsyn)*range(x$nminsyn)
  included=max(x$nbiomass)*max(x$nmaxsyn)*max(x$nminsyn) - excluded
  
  #stochastic portion
  points=data.frame(nbiomass=runif(num,
                                   min(x$nbiomass),
                                   max(x$nbiomass)),
                    nmaxsyn =runif(num,
                                   min(x$nmaxsyn),
                                   max(x$nmaxsyn)),
                    nminsyn =runif(num,
                                   min(x$nminsyn),
                                   max(x$nminsyn)))
  a=dominated(points,x)
  
  return(excluded*sum(a)/num + included)
}
@


\subsection{GDLS}
GDLS does not run to a hard limit, but instead stops when it finds the local optimum, so we cannot work out a useful upper bound on running time. 
A lower bound is, however, possible, and appears from exploratory experiments to be useful.
Four variables are used here to parametrize GDLS: 
\begin{enumerate}
\item The metabolic model used, let us call the number of potential knockouts in it \texttt{modelsize};
\item \texttt{nbhdsz}, number of knockouts distance from a solution that is exhaustively searched;
\item \texttt{M}, the number of search paths to maintain at each stage; and
\item \texttt{knockouts}, the maximum total number of knockouts (fixed at 40 for fair comparison with GDMO).
\end{enumerate}
Here we can see that the metabolic model used will have a very large effect, since not only does this effect the time to complete one FBA evaluation, but the space that much be searched around a given solution is \(\texttt{modelsize} ^ \texttt{neighbourhoodsize}\). This means that, total search time for one solution is roughly \(\texttt{modelsize}^{(3+\texttt{neighbourhoodsize})}\), once the time to conduct FBA is included. Overall, this gives a lower bound on running time of \(\texttt{M} * \texttt{modelsize} ^{ ( 3 + \texttt{neighbourhoodsize})}\).
\todo{is branching factor number of search paths, or is it at each stage?}

To find the relative effects of these parameters on execution time, the trials listed in table~\ref{tab:GDLSevals} were performed.
<<GDLSevals,results='asis'>>=
data(GDLS)
library(xtable,quietly=TRUE)
GDLS.runs=unique(GDLS[,c('strain','nbhdsz','M')])
print(xtable(
  GDLS.runs[,c('strain','nbhdsz','M')],
  caption='GDLS evaluations completed',
  label='tab:GDLSevals',
  display=c('d','s','d','d')
  ),include.rownames=FALSE)
@
\todo{need to actually make a prediction}

\section{Instrumentation}
To facilitate comparison, the genetic design strategies were instrumented in order to record every candidate solution that they produced. This allows the improvement of the candidates over time to be seen. 
Tests were conducted to ensure that this instrumentation code did not influence the results by its own execution time; it was found to run fast enough that the slowdown was not detectable above noise.
